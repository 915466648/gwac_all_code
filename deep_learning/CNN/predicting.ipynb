{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "scaler = preprocessing.MinMaxScaler((0,1))\n",
    "plt.rcParams['savefig.dpi'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.13 |Anaconda, Inc.| (default, Feb 23 2021, 21:15:04) \n",
      "[GCC 7.3.0]\n",
      "/home/wamdm/anaconda3/envs/tensorflowCPU/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_array(imageFile):\n",
    "    img = mpimg.imread(imageFile).astype(np.float)\n",
    "    img=img[:,:,0]\n",
    "    img=1-img\n",
    "    return img.reshape((50,50,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pool_1) 2633\n",
      "len(pool_0) 6872\n"
     ]
    }
   ],
   "source": [
    "path_0 = \"./GWAC_data/test_0/*.png\"\n",
    "# path_0 = \"./Finded_0909/*.png\"\n",
    "path_1 = \"./GWAC_data/test_1/*.png\"\n",
    "\n",
    "pool_0=[imageFile for imageFile in glob.glob(path_0)]\n",
    "pool_1=[imageFile for imageFile in glob.glob(path_1)]\n",
    "print(\"len(pool_1)\",len(pool_1))\n",
    "print(\"len(pool_0)\",len(pool_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_size=len(pool_1)+len(pool_0)\n",
    "x_test = np.zeros((test_sample_size, 50,50,1))\n",
    "y_test = np.zeros((test_sample_size,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 训练模型。但不明白其中的24和80是怎么回事，哪来的，可能是作者自己定义的。\n",
    "\n",
    "# [0,1]1 for fs\n",
    "# [1,0]0 for blank\n",
    "y0=np.array([1,0])\n",
    "y1=np.array([0,1])\n",
    "\n",
    "sample_monitor0=[]\n",
    "sample_monitor1=[]\n",
    "\n",
    "positive = []\n",
    "negative = []\n",
    "### 生成3000个样本，用来训练，其中三种类别的比例是一样的\n",
    "for i in range(test_sample_size):\n",
    "    ## 从各自的样本数量范围内，随机抽取一个整数\n",
    "    if i<len(pool_1):\n",
    "        x_test[i] = img_to_array(pool_1[i])\n",
    "        y_test[i] = y1\n",
    "        positive.append(i)\n",
    "    else:\n",
    "        x_test[i] = img_to_array(pool_0[i-len(pool_1)-1])\n",
    "        y_test[i] = y0\n",
    "        negative.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9505, 50, 50, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 2\n",
    "input_shape = (50,50,1)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb86d7d4a90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[160].reshape(50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 46, 46, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 23, 23, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 16)        1552      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 10, 10, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 32)          4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               61560     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 170       \n",
      "=================================================================\n",
      "Total params: 78,242\n",
      "Trainable params: 78,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#读取训练的模型\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"./LeNet_batch_size_100_epochs_3_kernel_size_543_accuracy_0.9944770336151123.h5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6842,   30],\n",
       "       [   7, 2626]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "true_y=np.apply_along_axis(np.argmax,1,y_test)\n",
    "\n",
    "pred_y=model.predict(x_test)\n",
    "pred_y=np.apply_along_axis(np.argmax,1,pred_y)\n",
    "\n",
    "confusion_mat = confusion_matrix(true_y, pred_y)\n",
    "\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb7f84780f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAFkElEQVR4nO3bMYueVRrG8eveBIs0sdgtdo0sKURIPfgZYiVsZdhSmMoPYLsfwiZFsFMsLQRbGwvtVlmEICxGllURK4sgHAtTZMPCvBOfd5441+/XPac4c8OZP+d5mXdmrRXgcvvD3gMAxyd0KCB0KCB0KCB0KCB0KCD0c5iZ2zPz5czcn5m39p6Hw83MvZn5dmY+33uWPQj9QDNzJcnbSV5NcivJnZm5te9UnMM7SW7vPcRehH64V5LcX2t9tdZ6mOS9JK/tPBMHWmt9nOSHvefYi9AP90KSrx97fvBoDZ55QocCQj/cN0lefOz5xqM1eOYJ/XCfJnlpZm7OzHNJXk/ywc4zwUGEfqC11s9J3kzyUZJ/JXl/rfXFvlNxqJl5N8knSV6emQcz88beM12k8W+qcPm50aGA0KGA0KGA0KGA0KGA0M9pZk73noGn13p+Qj+/yl+US6Ty/IQOBY7yhZlrM+v5zXd9NvyU5NreQxzZf/LnvUc4ost+gj9mrZ/mydWrx/hRz6f0/eiS+IfT+x27+39XvbpDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDAaFDgYNCn5nbM/PlzNyfmbeOPRSwrTNDn5krSd5O8mqSW0nuzMytYw8GbOeQG/2VJPfXWl+ttR4meS/Ja8cdC9jSIaG/kOTrx54fPFoDfieubrXRzJwmOU2S61ttCmzikBv9myQvPvZ849Ha/1hr3V1rnay1Tq5tNR2wiUNC/zTJSzNzc2aeS/J6kg+OOxawpTNf3ddaP8/Mm0k+SnIlyb211hdHnwzYzEGf0ddaHyb58MizAEfim3FQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQQOhQYNZa2286f1nJ6eb7cjFurL/vPQJP6b8nf8vDz/45T6670aGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KGA0KHAmaHPzL2Z+XZmPr+IgYDtHXKjv5Pk9pHnAI7ozNDXWh8n+eECZgGOxGd0KHB1q41m5jTJ6a9P17faFtjAZjf6WuvuWutkrXWSXNtqW2ADXt2hwCF/Xns3ySdJXp6ZBzPzxvHHArZ05mf0tdadixgEOB6v7lBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBA6FBg1lrbbzrzXZJ/b77xs+GPSb7fewie2mU/v7+utf705OJRQr/MZuaztdbJ3nPwdFrPz6s7FBA6FBD6+d3dewB+k8rz8xkdCrjRoYDQoYDQoYDQoYDQocAvhj+pMDWUwJAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(confusion_mat, cmap='jet')#viridis,jet_r,rainbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9990    0.9956    0.9973      6872\n",
      "           1     0.9887    0.9973    0.9930      2633\n",
      "\n",
      "    accuracy                         0.9961      9505\n",
      "   macro avg     0.9938    0.9965    0.9952      9505\n",
      "weighted avg     0.9961    0.9961    0.9961      9505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(true_y,pred_y,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48871187.73681304\n",
      "48871188.99158615\n",
      "1.254773110151291\n",
      "48871188.99172161\n",
      "48871190.27112259\n",
      "1.2794009819626808\n",
      "48871190.27290545\n",
      "48871192.006374635\n",
      "1.7334691882133484\n",
      "48871192.00685336\n",
      "48871193.25960965\n",
      "1.2527562901377678\n",
      "48871193.25998099\n",
      "48871194.50047734\n",
      "1.2404963448643684\n",
      "48871194.500950664\n",
      "48871195.74345692\n",
      "1.2425062581896782\n",
      "48871195.743918665\n",
      "48871198.0662283\n",
      "2.322309635579586\n",
      "48871198.06670854\n",
      "48871199.292305864\n",
      "1.225597321987152\n",
      "48871199.29269933\n",
      "48871200.871610805\n",
      "1.5789114758372307\n",
      "48871200.87170717\n",
      "48871202.29923846\n",
      "1.427531287074089\n",
      "[1.254773110151291, 1.2794009819626808, 1.7334691882133484, 1.2527562901377678, 1.2404963448643684, 1.2425062581896782, 2.322309635579586, 1.225597321987152, 1.5789114758372307, 1.427531287074089]\n",
      "1.4557751893997193\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "test_time_cnn = []\n",
    "for i in range(10):\n",
    "    start_time = timer()\n",
    "    print(start_time)\n",
    "    model.predict(x_test)\n",
    "    current_time = timer()\n",
    "    print(current_time)\n",
    "    test_time_cnn.append(current_time-start_time)\n",
    "    print(current_time-start_time)\n",
    "    i = i+1\n",
    "print(test_time_cnn)\n",
    "print(np.mean(test_time_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘制类激活热力图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def heatmapcam(img,i,path):\n",
    "    last_conv_layer='conv2d_5'\n",
    "\n",
    "#     predicts=model.predict_classes(img)\n",
    "    predicts=model.predict(img)\n",
    "#     print(\"predicts\",predicts)\n",
    "    print(np.argmax(predicts[0]),predicts[0].max())\n",
    "\n",
    "    # get the last conv layer\n",
    "    last_conv_layer = model.get_layer(last_conv_layer)\n",
    "\n",
    "    heatmap_model = models.Model([model.inputs], [last_conv_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as gtape:\n",
    "        conv_output, Predictions = heatmap_model(img)\n",
    "#         print(\"Predictions\",Predictions)\n",
    "        prob = Predictions[:, np.argmax(Predictions[0])] # 最大可能性类别的预测概率\n",
    "#         print(\"prob\",prob)\n",
    "        grads = gtape.gradient(prob, conv_output)  # 类别与卷积层的梯度 (1,14,14,512)\n",
    "        pooled_grads = K.mean(grads, axis=(0,1,2)) # 特征层梯度的全局平均代表每个特征层权重\n",
    "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1) #权重与特征层相乘，512层求和平均\n",
    "\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    max_heat = np.max(heatmap)\n",
    "    if max_heat == 0:\n",
    "        max_heat = 1e-10\n",
    "    heatmap /= max_heat\n",
    "    # plt.matshow(heatmap[0], cmap='jet')#viridis,jet_r,rainbow\n",
    "\n",
    "\n",
    "    heatmap_sample=heatmap[0]\n",
    "#     print(\"heatmap_sample\",heatmap_sample)\n",
    "    cov_size=10\n",
    "    x = np.arange(0, cov_size, 1)\n",
    "    y = np.arange(0, cov_size, 1)\n",
    "\n",
    "    f = interpolate.interp2d(x, y, heatmap_sample, kind='cubic')\n",
    "\n",
    "    xnew = np.linspace(0, cov_size, 50)\n",
    "    ynew = np.linspace(0, cov_size, 50)\n",
    "    znew = f(xnew, ynew)\n",
    "\n",
    "    ## 画出热力图\n",
    "    plt.matshow(znew, cmap='jet')#viridis,jet_r,rainbow\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + str(i) + '_'+ str(np.argmax(predicts[0])) +'_'+ str(predicts[0].max())+'.png')\n",
    "    ## cmap='Greys_r'表示画出的图为灰度图\n",
    "    ## 叠加原图\n",
    "    plt.imshow(img.reshape(50,50),cmap='Greys_r',alpha=0.5)\n",
    "    plt.savefig(path + str(i) + '_'+ str(np.argmax(predicts[0])) +'_'+ str(predicts[0].max())+'.png')\n",
    "#     plt.clf()#添加上这一行，画完图后，重置一下\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5600283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wamdm/anaconda3/envs/tensorflowCPU/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9520607\n",
      "1 0.97341114\n",
      "1 0.957182\n",
      "1 0.9495192\n",
      "1 0.76376235\n",
      "1 0.9413892\n",
      "1 0.92256105\n",
      "1 0.9198393\n",
      "1 0.97447586\n",
      "1 0.9773219\n",
      "1 0.9768404\n",
      "1 0.9540025\n",
      "0 0.6376034\n",
      "1 0.9291062\n",
      "1 0.9338095\n",
      "1 0.9595991\n",
      "1 0.9480663\n",
      "1 0.9480842\n",
      "1 0.77135384\n",
      "1 0.9495151\n",
      "1 0.94167143\n",
      "1 0.9273497\n",
      "1 0.9363264\n",
      "0 0.5824229\n",
      "1 0.8275043\n",
      "1 0.9496888\n",
      "1 0.6867075\n",
      "1 0.84304905\n",
      "1 0.93520665\n",
      "1 0.54569936\n",
      "1 0.7746277\n",
      "1 0.963384\n",
      "1 0.6643507\n",
      "1 0.899038\n",
      "1 0.93269837\n",
      "1 0.90392834\n",
      "1 0.92726165\n",
      "1 0.9597799\n",
      "1 0.78684217\n",
      "1 0.7556999\n",
      "1 0.8125866\n",
      "1 0.9756433\n",
      "1 0.97039807\n",
      "1 0.9367085\n",
      "1 0.9507638\n",
      "1 0.6769377\n",
      "1 0.93797594\n",
      "1 0.93107617\n",
      "1 0.9433653\n",
      "1 0.90147626\n",
      "1 0.924655\n",
      "1 0.9461694\n",
      "1 0.9455588\n",
      "1 0.95733666\n",
      "1 0.9593051\n",
      "1 0.5728224\n",
      "0 0.6237099\n",
      "1 0.92310876\n",
      "1 0.96522886\n",
      "1 0.81083566\n",
      "0 0.6792867\n",
      "1 0.53429693\n",
      "1 0.8294291\n",
      "1 0.9550157\n",
      "0 0.56009346\n",
      "1 0.9522717\n",
      "1 0.95166314\n",
      "1 0.5390028\n",
      "1 0.9369672\n",
      "1 0.96380216\n",
      "1 0.92920244\n",
      "1 0.66574097\n",
      "1 0.9344658\n",
      "1 0.9662375\n",
      "1 0.9615537\n",
      "1 0.9121355\n",
      "1 0.809763\n",
      "1 0.58810115\n",
      "1 0.9509615\n",
      "1 0.5394692\n",
      "1 0.9438099\n",
      "1 0.935625\n",
      "1 0.9276989\n",
      "1 0.96006817\n",
      "0 0.58634764\n",
      "1 0.9610058\n",
      "1 0.9573714\n",
      "1 0.9500342\n",
      "1 0.9699094\n",
      "1 0.75049984\n",
      "1 0.9258973\n",
      "1 0.9218361\n",
      "0 0.6454173\n",
      "0 0.64076704\n",
      "0 0.67777556\n",
      "1 0.516016\n",
      "1 0.78790146\n",
      "1 0.51258415\n",
      "1 0.9496888\n",
      "1 0.96550995\n",
      "1 0.884107\n",
      "1 0.94087553\n",
      "1 0.92891985\n",
      "1 0.7754092\n",
      "1 0.92765635\n",
      "1 0.93800515\n",
      "1 0.9376017\n",
      "1 0.9528758\n",
      "1 0.95895725\n",
      "1 0.73611647\n",
      "1 0.7468192\n",
      "1 0.9160125\n",
      "1 0.89728135\n",
      "1 0.9295592\n",
      "1 0.9325976\n",
      "1 0.8406997\n",
      "1 0.92351437\n",
      "1 0.8942407\n",
      "0 0.84454614\n",
      "0 0.54304177\n",
      "1 0.9713382\n",
      "1 0.9379594\n",
      "1 0.9420279\n",
      "0 0.9983335\n",
      "1 0.9003367\n",
      "1 0.67495304\n",
      "1 0.9544983\n",
      "1 0.9403354\n",
      "1 0.9302291\n",
      "1 0.5196724\n",
      "1 0.9118565\n",
      "1 0.9563235\n",
      "1 0.9276989\n",
      "1 0.9751589\n",
      "1 0.7985558\n",
      "0 0.5824229\n",
      "0 0.65707374\n",
      "1 0.839979\n",
      "1 0.91004825\n",
      "1 0.6609528\n",
      "0 0.71384233\n",
      "1 0.73378396\n",
      "1 0.9326971\n",
      "1 0.93227845\n",
      "1 0.92639625\n",
      "1 0.9408428\n",
      "1 0.9421113\n",
      "1 0.9455588\n",
      "0 0.60766345\n",
      "1 0.835839\n",
      "1 0.71978104\n",
      "1 0.9666353\n",
      "1 0.95529777\n",
      "1 0.93587154\n",
      "1 0.6912695\n",
      "1 0.75799775\n",
      "1 0.9596112\n",
      "1 0.9490769\n",
      "1 0.95515805\n",
      "1 0.6587469\n",
      "1 0.9586436\n",
      "1 0.69415706\n",
      "0 0.9967513\n",
      "0 0.55078936\n",
      "1 0.97198796\n",
      "1 0.82834244\n",
      "1 0.937358\n",
      "1 0.5344554\n",
      "1 0.95794845\n",
      "1 0.93317163\n",
      "1 0.95568705\n",
      "1 0.9553479\n",
      "1 0.79903704\n",
      "1 0.96043235\n",
      "1 0.9615359\n",
      "1 0.9470349\n",
      "1 0.926478\n",
      "0 0.63744044\n",
      "1 0.9235073\n",
      "0 0.99726665\n",
      "1 0.9501013\n",
      "1 0.70017916\n",
      "1 0.95879406\n",
      "1 0.93387276\n",
      "1 0.9549818\n",
      "1 0.5336681\n",
      "1 0.958629\n",
      "1 0.90196836\n",
      "1 0.9625015\n",
      "1 0.538977\n",
      "1 0.9094801\n",
      "0 0.542147\n",
      "1 0.955141\n",
      "1 0.82356095\n",
      "1 0.9675368\n",
      "0 0.507018\n",
      "1 0.9714877\n",
      "0 0.55078936\n",
      "1 0.96142554\n",
      "1 0.9678652\n",
      "1 0.9755558\n",
      "1 0.9519465\n",
      "0 0.7527678\n",
      "1 0.5469925\n",
      "1 0.8445119\n",
      "1 0.9532599\n",
      "1 0.64633393\n",
      "1 0.57079023\n",
      "1 0.9519445\n",
      "1 0.9584388\n",
      "1 0.54901683\n",
      "1 0.58102536\n",
      "1 0.97523695\n",
      "1 0.5226027\n",
      "0 0.50834274\n",
      "1 0.971183\n",
      "1 0.8538286\n",
      "1 0.9544529\n",
      "1 0.90665025\n",
      "1 0.55000097\n",
      "1 0.9528009\n",
      "1 0.93125033\n",
      "1 0.9179925\n",
      "1 0.91628593\n",
      "1 0.9661003\n",
      "1 0.51155776\n",
      "1 0.835839\n",
      "0 0.596121\n",
      "1 0.94311553\n",
      "1 0.9223457\n",
      "1 0.96241266\n",
      "1 0.6687068\n",
      "1 0.7323054\n",
      "1 0.7048497\n",
      "1 0.800049\n",
      "1 0.96177053\n",
      "1 0.9544305\n",
      "1 0.95762604\n",
      "1 0.94993985\n",
      "1 0.9704047\n",
      "1 0.9446107\n",
      "1 0.9676876\n",
      "1 0.9610572\n",
      "1 0.957221\n",
      "1 0.9665754\n",
      "0 0.9953478\n",
      "1 0.96577924\n",
      "1 0.95774674\n",
      "1 0.9363193\n",
      "1 0.93619776\n",
      "0 0.9982021\n",
      "1 0.9625456\n",
      "1 0.92378885\n",
      "1 0.94957876\n",
      "0 0.6735793\n",
      "1 0.95396954\n",
      "0 0.5040164\n",
      "1 0.611333\n",
      "1 0.9545138\n",
      "1 0.96661466\n",
      "1 0.93803924\n",
      "1 0.89306855\n",
      "1 0.9623855\n",
      "1 0.95644087\n",
      "1 0.97484684\n",
      "1 0.95115006\n",
      "1 0.9593889\n",
      "1 0.8125866\n",
      "1 0.9494003\n",
      "1 0.9536874\n",
      "1 0.9468206\n",
      "1 0.95895725\n",
      "0 0.85845554\n",
      "1 0.95691603\n",
      "1 0.80547094\n",
      "1 0.9639109\n",
      "1 0.95042676\n",
      "1 0.9650079\n",
      "1 0.91059643\n",
      "1 0.90607715\n",
      "1 0.92086\n",
      "1 0.95691603\n",
      "1 0.95326316\n",
      "1 0.9757244\n",
      "0 0.8427827\n",
      "1 0.96308464\n",
      "1 0.9584272\n",
      "1 0.64095\n",
      "1 0.93747175\n",
      "1 0.9532921\n",
      "1 0.96018577\n",
      "1 0.9761233\n",
      "1 0.6293662\n",
      "0 0.605846\n",
      "1 0.957182\n",
      "1 0.92736644\n",
      "1 0.95132434\n",
      "1 0.89644504\n",
      "1 0.9602849\n",
      "1 0.89575887\n",
      "1 0.6906944\n",
      "1 0.7037523\n",
      "1 0.9148534\n",
      "1 0.89737904\n",
      "1 0.9622791\n",
      "1 0.9121355\n",
      "1 0.8944064\n",
      "1 0.9533547\n",
      "1 0.92351437\n",
      "1 0.9317902\n",
      "1 0.94971377\n",
      "1 0.9509537\n",
      "1 0.9657476\n",
      "1 0.9407265\n",
      "1 0.95152485\n",
      "1 0.94285816\n",
      "1 0.9500416\n",
      "1 0.9243708\n",
      "1 0.6906944\n",
      "1 0.7901918\n",
      "1 0.96348023\n",
      "1 0.7956803\n",
      "1 0.9631331\n",
      "1 0.904056\n",
      "1 0.7938252\n",
      "1 0.7271385\n",
      "1 0.9740426\n",
      "1 0.93306303\n",
      "1 0.93477464\n",
      "1 0.9264438\n",
      "1 0.95197266\n",
      "1 0.7919912\n",
      "1 0.9588024\n",
      "1 0.92882586\n",
      "1 0.50568736\n",
      "1 0.9503615\n",
      "1 0.93255186\n",
      "1 0.95510066\n",
      "1 0.64854825\n",
      "0 0.6051709\n",
      "1 0.60869473\n",
      "1 0.9281127\n",
      "1 0.8360022\n",
      "1 0.7211784\n",
      "1 0.96710557\n",
      "1 0.9575353\n",
      "1 0.9363264\n",
      "1 0.9089071\n",
      "1 0.959792\n",
      "1 0.97588646\n",
      "1 0.5895152\n",
      "1 0.5817672\n",
      "1 0.96809846\n",
      "1 0.90949845\n",
      "1 0.95940584\n",
      "1 0.95762604\n",
      "1 0.9580041\n",
      "1 0.9016208\n",
      "1 0.963878\n",
      "1 0.50493115\n",
      "1 0.95162094\n",
      "1 0.9651168\n",
      "1 0.93626535\n",
      "0 0.9978399\n",
      "1 0.9761879\n",
      "1 0.91342056\n",
      "1 0.9563235\n",
      "1 0.63803464\n",
      "1 0.8244278\n",
      "1 0.9640469\n",
      "1 0.9563691\n",
      "1 0.7988223\n",
      "1 0.9207823\n",
      "1 0.95580465\n",
      "1 0.97013724\n",
      "1 0.7756118\n",
      "1 0.95477897\n",
      "0 0.6562576\n",
      "1 0.9508315\n",
      "1 0.9324865\n",
      "1 0.9725605\n",
      "1 0.9604705\n",
      "1 0.93915117\n",
      "1 0.7566668\n",
      "1 0.95865005\n",
      "1 0.70957893\n",
      "1 0.96327055\n",
      "1 0.8909975\n",
      "1 0.95046365\n",
      "1 0.5966725\n",
      "1 0.9413077\n",
      "1 0.8788245\n",
      "1 0.94907415\n",
      "0 0.7111561\n",
      "1 0.97245556\n",
      "1 0.93428963\n",
      "1 0.9038346\n",
      "1 0.9221828\n",
      "1 0.93102854\n",
      "1 0.69861454\n",
      "1 0.9676876\n",
      "1 0.92658883\n",
      "1 0.95641154\n",
      "1 0.9266532\n",
      "1 0.9622791\n",
      "0 0.9967513\n",
      "1 0.96315587\n",
      "1 0.9504514\n",
      "1 0.73378396\n",
      "1 0.9223457\n",
      "0 0.5705201\n",
      "1 0.9493867\n",
      "1 0.937211\n",
      "1 0.93302184\n",
      "1 0.5150943\n",
      "1 0.88289744\n",
      "1 0.95907545\n",
      "1 0.5805035\n",
      "1 0.9400439\n",
      "1 0.9094801\n",
      "1 0.956031\n",
      "1 0.9000775\n",
      "1 0.93524456\n",
      "1 0.9432266\n",
      "1 0.9646367\n",
      "1 0.9094456\n",
      "1 0.9591614\n",
      "1 0.9559056\n",
      "1 0.96821666\n",
      "1 0.68433046\n",
      "1 0.6227806\n",
      "1 0.96598536\n",
      "1 0.7397692\n",
      "1 0.91496813\n",
      "1 0.9676092\n",
      "1 0.94429064\n",
      "1 0.91874355\n",
      "1 0.9745746\n",
      "1 0.9525494\n",
      "1 0.9482875\n",
      "1 0.93795764\n",
      "1 0.9079836\n",
      "1 0.9631331\n",
      "1 0.95525205\n",
      "1 0.94523966\n",
      "1 0.9442981\n",
      "1 0.94089323\n",
      "1 0.93270105\n",
      "0 0.99837506\n",
      "1 0.92909807\n",
      "1 0.7981235\n",
      "1 0.9554481\n",
      "1 0.93859065\n",
      "1 0.94337314\n",
      "1 0.91628593\n",
      "1 0.8189425\n",
      "1 0.97466403\n",
      "1 0.96709055\n",
      "1 0.963868\n",
      "1 0.9767616\n",
      "1 0.80703217\n",
      "1 0.7919912\n",
      "0 0.93700874\n",
      "1 0.9519445\n",
      "1 0.9595644\n",
      "1 0.9183825\n",
      "1 0.9579444\n",
      "1 0.9020098\n",
      "0 0.9973218\n",
      "1 0.6873509\n",
      "1 0.96710736\n",
      "1 0.91610366\n",
      "1 0.8538286\n",
      "1 0.93303925\n",
      "1 0.96032304\n",
      "1 0.6979515\n",
      "1 0.9363243\n",
      "1 0.89306855\n",
      "1 0.9302291\n",
      "1 0.9602474\n",
      "0 0.6125709\n",
      "1 0.9500416\n",
      "1 0.97628206\n",
      "1 0.6811422\n",
      "1 0.820868\n",
      "1 0.9554283\n",
      "1 0.93777674\n",
      "1 0.9370581\n",
      "1 0.9588838\n",
      "1 0.67968136\n",
      "1 0.9634735\n",
      "1 0.92535603\n",
      "1 0.5391365\n",
      "1 0.95732695\n",
      "1 0.9334834\n",
      "1 0.91759783\n",
      "1 0.92830276\n",
      "1 0.59433144\n",
      "1 0.6944332\n",
      "1 0.92726535\n",
      "0 0.5824229\n",
      "1 0.8633715\n",
      "1 0.9659959\n",
      "1 0.95863473\n",
      "1 0.89737904\n",
      "1 0.79903704\n",
      "1 0.949251\n",
      "0 0.5971419\n",
      "1 0.6286703\n",
      "1 0.963868\n",
      "1 0.59098244\n",
      "1 0.7451264\n",
      "1 0.95652753\n",
      "1 0.9172841\n",
      "1 0.9493783\n",
      "1 0.9575946\n",
      "1 0.8204156\n",
      "1 0.96710736\n",
      "1 0.9255868\n",
      "1 0.9362334\n",
      "1 0.9561985\n",
      "0 0.52300805\n",
      "1 0.9713382\n",
      "1 0.67524195\n",
      "1 0.97710514\n",
      "1 0.78790146\n",
      "1 0.92095953\n",
      "1 0.54864484\n",
      "1 0.9219892\n",
      "1 0.96348023\n",
      "1 0.78394437\n",
      "1 0.9337043\n",
      "1 0.9367085\n",
      "1 0.834704\n",
      "1 0.9574067\n",
      "1 0.96577704\n",
      "1 0.89331466\n",
      "1 0.92461586\n",
      "1 0.8150192\n",
      "1 0.9333071\n",
      "1 0.9678652\n",
      "1 0.9084233\n",
      "0 0.9979134\n",
      "1 0.8994734\n",
      "1 0.9603251\n",
      "1 0.95661426\n",
      "1 0.6640626\n",
      "1 0.5282984\n",
      "1 0.839979\n",
      "1 0.967071\n",
      "1 0.9446107\n",
      "1 0.5391365\n",
      "1 0.9744178\n",
      "1 0.9415827\n",
      "1 0.9532599\n",
      "1 0.9495056\n",
      "1 0.9292925\n",
      "1 0.9480663\n",
      "1 0.9676549\n",
      "1 0.9286838\n",
      "0 0.6454173\n",
      "1 0.90350884\n",
      "1 0.96248066\n",
      "1 0.97466403\n",
      "1 0.96656466\n",
      "1 0.82342166\n",
      "1 0.9651168\n",
      "1 0.94958425\n",
      "1 0.9499483\n",
      "1 0.90496427\n",
      "1 0.9514492\n",
      "1 0.9639358\n",
      "1 0.95452356\n",
      "1 0.94096625\n",
      "1 0.7104144\n",
      "1 0.95808774\n",
      "1 0.9375747\n",
      "1 0.9610526\n",
      "1 0.8367365\n",
      "1 0.94907415\n",
      "1 0.95893025\n",
      "1 0.95691603\n",
      "1 0.8887636\n",
      "1 0.9348998\n",
      "1 0.9640759\n",
      "1 0.96714413\n",
      "1 0.94363886\n",
      "1 0.9603251\n",
      "1 0.7446871\n",
      "1 0.91539717\n",
      "1 0.96714413\n",
      "1 0.78868043\n",
      "1 0.93509495\n",
      "0 0.52332824\n",
      "1 0.95863473\n",
      "0 0.6237099\n",
      "1 0.66644263\n",
      "1 0.9483831\n",
      "1 0.92927825\n",
      "1 0.94908667\n",
      "0 0.8273928\n",
      "1 0.93980527\n",
      "1 0.9596661\n",
      "1 0.7942811\n",
      "1 0.9512892\n",
      "1 0.95743513\n",
      "1 0.9738482\n",
      "1 0.77994114\n",
      "1 0.9216221\n",
      "1 0.77523124\n",
      "1 0.7898371\n",
      "1 0.94875616\n",
      "1 0.9555216\n",
      "1 0.9320399\n",
      "1 0.9630505\n",
      "1 0.94871646\n",
      "1 0.9669387\n",
      "1 0.9152778\n",
      "1 0.95644087\n",
      "1 0.5793222\n",
      "1 0.8898741\n",
      "1 0.956499\n",
      "1 0.6125423\n",
      "1 0.9514745\n",
      "1 0.5262792\n",
      "1 0.93821084\n",
      "1 0.9544983\n",
      "1 0.96684945\n",
      "0 0.6941558\n",
      "1 0.9076449\n",
      "1 0.6867075\n",
      "1 0.92863697\n",
      "1 0.9593889\n",
      "1 0.7985558\n",
      "1 0.9412058\n",
      "1 0.91759783\n",
      "0 0.99681574\n",
      "1 0.9588024\n",
      "1 0.91496813\n",
      "1 0.89437264\n",
      "1 0.74096674\n",
      "1 0.9433653\n",
      "1 0.9265218\n",
      "1 0.95458376\n",
      "1 0.9522717\n",
      "1 0.9662605\n",
      "1 0.90342546\n",
      "1 0.97829324\n",
      "1 0.9351983\n",
      "1 0.9325976\n",
      "1 0.96168035\n",
      "0 0.50679463\n",
      "0 0.67185456\n",
      "0 0.95475227\n",
      "1 0.95946574\n",
      "1 0.95042676\n",
      "1 0.9408034\n",
      "1 0.7946331\n",
      "1 0.92578924\n",
      "1 0.8529233\n",
      "1 0.9776403\n",
      "1 0.9208369\n",
      "1 0.9563319\n",
      "1 0.80226517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.93198484\n",
      "1 0.7670948\n",
      "1 0.93748194\n",
      "1 0.88975084\n",
      "1 0.9175596\n",
      "1 0.956031\n",
      "1 0.9610058\n",
      "1 0.6758184\n",
      "1 0.93412405\n",
      "1 0.5464178\n",
      "1 0.9757244\n",
      "0 0.51672196\n",
      "1 0.71388936\n",
      "1 0.9128564\n",
      "1 0.6699188\n",
      "1 0.9197628\n",
      "1 0.9326971\n",
      "0 0.50679463\n",
      "1 0.942211\n",
      "1 0.96629626\n",
      "1 0.9413077\n",
      "0 0.5182957\n",
      "0 0.9058533\n",
      "1 0.8133509\n",
      "1 0.6999798\n",
      "1 0.92624485\n",
      "1 0.965876\n",
      "1 0.9560641\n",
      "1 0.8539243\n",
      "1 0.5471771\n",
      "1 0.93776613\n",
      "1 0.95940584\n",
      "1 0.93587154\n",
      "1 0.912557\n",
      "1 0.9661827\n",
      "1 0.73979867\n",
      "1 0.93886775\n",
      "1 0.97573894\n",
      "1 0.96453786\n",
      "0 0.6215333\n",
      "1 0.9615359\n",
      "1 0.90985507\n",
      "1 0.951252\n",
      "1 0.51353246\n",
      "1 0.9166997\n",
      "0 0.8273928\n",
      "1 0.95438457\n",
      "1 0.9597799\n",
      "1 0.9418161\n",
      "1 0.9760311\n",
      "1 0.67195994\n",
      "1 0.6769377\n",
      "1 0.6875036\n",
      "1 0.90389174\n",
      "1 0.9428304\n",
      "1 0.9595726\n",
      "1 0.5344944\n",
      "1 0.95743513\n",
      "1 0.95539206\n",
      "1 0.96308464\n",
      "1 0.9517592\n",
      "1 0.9470357\n",
      "1 0.92440814\n",
      "1 0.9684705\n",
      "1 0.91168314\n",
      "1 0.95757973\n",
      "1 0.9559056\n",
      "1 0.97553647\n",
      "1 0.94963366\n",
      "1 0.73091644\n",
      "1 0.9544535\n",
      "1 0.9462698\n",
      "1 0.82023716\n",
      "1 0.96550995\n",
      "1 0.96373516\n",
      "1 0.9505186\n",
      "1 0.9593739\n",
      "1 0.94326687\n",
      "1 0.50236785\n",
      "1 0.7779134\n",
      "1 0.897036\n",
      "1 0.9598459\n",
      "1 0.9338095\n",
      "0 0.5149222\n",
      "1 0.93525594\n",
      "1 0.9595644\n",
      "1 0.8293592\n",
      "1 0.9545138\n",
      "1 0.96073043\n",
      "1 0.9422752\n",
      "1 0.9573714\n",
      "1 0.912386\n",
      "1 0.8302272\n",
      "1 0.9497667\n",
      "1 0.97574514\n",
      "1 0.9594528\n",
      "1 0.9751415\n",
      "1 0.9367042\n",
      "1 0.93317163\n",
      "1 0.9384066\n",
      "1 0.96684945\n",
      "1 0.9237729\n",
      "1 0.9522717\n",
      "1 0.94296265\n",
      "1 0.9615359\n",
      "1 0.8691696\n",
      "1 0.93779224\n",
      "1 0.94480455\n",
      "1 0.94681555\n",
      "1 0.6906369\n",
      "1 0.5913501\n",
      "1 0.94242436\n",
      "1 0.77181524\n",
      "0 0.6528363\n",
      "0 0.6735793\n",
      "1 0.5196724\n",
      "1 0.9302495\n",
      "1 0.7930335\n",
      "1 0.9440798\n",
      "1 0.91692805\n",
      "0 0.50679463\n",
      "1 0.9493873\n",
      "1 0.9332752\n",
      "1 0.9523724\n",
      "1 0.9173331\n",
      "1 0.53548557\n",
      "1 0.91616434\n",
      "1 0.9187509\n",
      "1 0.94227976\n",
      "1 0.96368587\n",
      "1 0.9151944\n",
      "0 0.6024643\n",
      "1 0.91389966\n",
      "1 0.86249685\n",
      "1 0.93056643\n",
      "1 0.9158754\n",
      "1 0.91129756\n",
      "1 0.8211711\n",
      "1 0.9433485\n",
      "1 0.9373517\n",
      "1 0.85540426\n",
      "1 0.7072764\n",
      "1 0.9613105\n",
      "1 0.91616434\n",
      "1 0.62204385\n",
      "1 0.9560641\n",
      "1 0.7687002\n",
      "1 0.8538286\n",
      "1 0.94945276\n",
      "0 0.6255187\n",
      "1 0.9606684\n",
      "1 0.9597799\n",
      "1 0.8977052\n",
      "1 0.97328883\n",
      "1 0.94326687\n",
      "1 0.83642846\n",
      "1 0.95999277\n",
      "1 0.94184715\n",
      "1 0.95988977\n",
      "1 0.9685287\n",
      "0 0.63090414\n",
      "1 0.9148534\n",
      "1 0.50942683\n",
      "1 0.9745676\n",
      "1 0.54901683\n",
      "1 0.9592315\n",
      "1 0.96668607\n",
      "1 0.6750701\n",
      "1 0.9367085\n",
      "1 0.9646367\n",
      "1 0.9214025\n",
      "1 0.90686446\n",
      "1 0.9094801\n",
      "1 0.9477922\n",
      "1 0.72649133\n",
      "1 0.82413644\n",
      "1 0.97155625\n",
      "1 0.9302514\n",
      "1 0.70299274\n",
      "1 0.9291628\n",
      "1 0.9351003\n",
      "1 0.9348727\n",
      "1 0.9309133\n",
      "1 0.96873635\n",
      "1 0.94868654\n",
      "1 0.937211\n",
      "1 0.9453514\n",
      "1 0.94510645\n",
      "1 0.95841646\n",
      "1 0.95975864\n",
      "1 0.899869\n",
      "1 0.8672793\n",
      "1 0.9329513\n",
      "1 0.70957893\n",
      "1 0.51407754\n",
      "1 0.91759783\n",
      "1 0.9613105\n",
      "1 0.95860744\n",
      "1 0.8045365\n",
      "1 0.84011394\n",
      "1 0.91895044\n",
      "1 0.9756757\n",
      "1 0.9280367\n",
      "1 0.68829155\n",
      "1 0.68902266\n",
      "1 0.9534519\n",
      "1 0.95596486\n",
      "1 0.93508476\n",
      "1 0.78505486\n",
      "1 0.8572479\n",
      "0 0.9961849\n",
      "1 0.6201478\n",
      "1 0.9619936\n",
      "1 0.61204404\n",
      "1 0.9506371\n",
      "1 0.9465941\n",
      "1 0.71365106\n",
      "1 0.80306935\n",
      "1 0.9704047\n",
      "1 0.9499579\n",
      "1 0.9334883\n",
      "1 0.9544268\n",
      "1 0.93661565\n",
      "1 0.91386193\n",
      "0 0.6133792\n",
      "1 0.9446107\n",
      "1 0.918232\n",
      "1 0.94040173\n",
      "1 0.6780912\n",
      "1 0.93821084\n",
      "1 0.6699188\n",
      "1 0.9281515\n",
      "1 0.950157\n",
      "1 0.9661796\n",
      "1 0.5118803\n",
      "1 0.9584272\n",
      "1 0.943525\n",
      "1 0.8285943\n",
      "1 0.924541\n",
      "1 0.6326091\n",
      "1 0.95999277\n",
      "1 0.96798843\n",
      "1 0.93419635\n",
      "1 0.9197176\n",
      "1 0.9217936\n",
      "1 0.9558664\n",
      "1 0.52486694\n",
      "1 0.9257168\n",
      "1 0.55024046\n",
      "0 0.73898184\n",
      "1 0.6873509\n",
      "1 0.9517963\n",
      "1 0.95799965\n",
      "1 0.95719326\n",
      "1 0.8621976\n",
      "1 0.68742937\n",
      "1 0.9503996\n",
      "1 0.9381177\n",
      "1 0.94893914\n",
      "1 0.8247376\n",
      "0 0.8401655\n",
      "1 0.95834136\n",
      "1 0.9613105\n",
      "1 0.92635065\n",
      "1 0.884523\n",
      "1 0.9560641\n",
      "1 0.96327055\n",
      "1 0.93795764\n",
      "1 0.95938826\n",
      "1 0.95515805\n",
      "1 0.78394437\n",
      "1 0.94793695\n",
      "1 0.9333071\n",
      "1 0.81550354\n",
      "1 0.9707235\n",
      "1 0.9515094\n",
      "1 0.9659414\n",
      "1 0.9458603\n",
      "1 0.934569\n",
      "1 0.93524456\n",
      "1 0.5526954\n",
      "0 0.6350626\n",
      "1 0.9244403\n",
      "1 0.96791637\n",
      "1 0.95690125\n",
      "1 0.9531325\n",
      "1 0.8498193\n",
      "1 0.82445663\n",
      "1 0.97223705\n",
      "1 0.96373516\n",
      "1 0.9474246\n",
      "1 0.9447902\n",
      "1 0.95606685\n",
      "1 0.71786827\n",
      "1 0.9224535\n",
      "1 0.90979666\n",
      "1 0.9166772\n",
      "1 0.9504053\n",
      "1 0.5913501\n",
      "0 0.7747589\n",
      "1 0.9580041\n",
      "1 0.9568131\n",
      "1 0.9428335\n",
      "1 0.93877476\n",
      "1 0.79115313\n",
      "1 0.92968965\n",
      "1 0.9368411\n",
      "1 0.9283859\n",
      "1 0.9419411\n",
      "1 0.9428539\n",
      "1 0.79107386\n",
      "1 0.95124173\n",
      "1 0.9438099\n",
      "1 0.95637673\n",
      "1 0.9059506\n",
      "1 0.83312374\n",
      "1 0.8150192\n",
      "1 0.9074057\n",
      "0 0.9978777\n",
      "1 0.9386608\n",
      "1 0.8114563\n",
      "0 0.8009955\n",
      "0 0.9973111\n",
      "1 0.9249204\n",
      "1 0.96562874\n",
      "1 0.91494\n",
      "1 0.9603251\n",
      "1 0.61564595\n",
      "1 0.9572308\n",
      "1 0.9483906\n",
      "1 0.9524711\n",
      "1 0.96562874\n",
      "1 0.95860744\n",
      "1 0.95046365\n",
      "1 0.8431085\n",
      "1 0.9636891\n",
      "0 0.5006271\n",
      "1 0.9273324\n",
      "0 0.67777556\n",
      "1 0.95999277\n",
      "1 0.5196724\n",
      "1 0.9428304\n",
      "1 0.9655737\n",
      "1 0.9610573\n",
      "1 0.9596112\n",
      "1 0.8040537\n",
      "1 0.97607034\n",
      "1 0.97835565\n",
      "1 0.9476795\n",
      "1 0.96319383\n",
      "1 0.9197134\n",
      "0 0.50679463\n",
      "1 0.6373583\n",
      "1 0.9397154\n",
      "0 0.6454173\n",
      "1 0.7581662\n",
      "1 0.8501392\n",
      "1 0.93836534\n",
      "1 0.9505857\n",
      "1 0.63956034\n",
      "1 0.9369089\n",
      "1 0.937358\n",
      "1 0.91386193\n",
      "0 0.6132287\n",
      "1 0.96253645\n",
      "1 0.9573229\n",
      "0 0.6081124\n",
      "1 0.7346375\n",
      "1 0.9650079\n",
      "1 0.79775995\n",
      "1 0.9332351\n",
      "1 0.916565\n",
      "1 0.9597799\n",
      "1 0.8431085\n",
      "1 0.9159085\n",
      "1 0.91955084\n",
      "1 0.93964267\n",
      "1 0.8197157\n",
      "1 0.960913\n",
      "1 0.953192\n",
      "1 0.93499553\n",
      "1 0.9518034\n",
      "1 0.734146\n",
      "1 0.96809846\n",
      "1 0.96283096\n",
      "1 0.9544305\n",
      "1 0.9035162\n",
      "1 0.9266532\n",
      "1 0.95634776\n",
      "1 0.9264438\n",
      "1 0.7919912\n",
      "1 0.9432266\n",
      "1 0.9669891\n",
      "1 0.9573229\n",
      "1 0.6506173\n",
      "1 0.93499786\n",
      "1 0.9452406\n",
      "1 0.8270915\n",
      "1 0.9233934\n",
      "1 0.9287847\n",
      "1 0.9655114\n",
      "1 0.51407754\n",
      "1 0.97392184\n",
      "1 0.81083566\n",
      "1 0.91926837\n",
      "1 0.9255446\n",
      "1 0.91953075\n",
      "1 0.94429064\n",
      "1 0.96026015\n",
      "1 0.9473051\n",
      "1 0.9057056\n",
      "1 0.9436508\n",
      "1 0.9093575\n",
      "1 0.963617\n",
      "1 0.95652056\n",
      "0 0.64967114\n",
      "1 0.9122723\n",
      "1 0.9163235\n",
      "1 0.8413013\n",
      "1 0.5779345\n",
      "1 0.9479086\n",
      "1 0.6227806\n",
      "1 0.8445119\n",
      "1 0.92097616\n",
      "1 0.81847465\n",
      "1 0.92871284\n",
      "1 0.8139859\n",
      "1 0.93938285\n",
      "1 0.9551084\n",
      "1 0.93303925\n",
      "1 0.7247546\n",
      "1 0.5332182\n",
      "1 0.795378\n",
      "1 0.9114867\n",
      "1 0.7919912\n",
      "1 0.97122675\n",
      "1 0.82231545\n",
      "1 0.9503337\n",
      "1 0.94615734\n",
      "1 0.9333827\n",
      "1 0.7936921\n",
      "1 0.7892088\n",
      "1 0.96837157\n",
      "1 0.92726165\n",
      "1 0.9544535\n",
      "1 0.9666716\n",
      "1 0.91759783\n",
      "1 0.774681\n",
      "1 0.9617042\n",
      "1 0.88817745\n",
      "1 0.9639358\n",
      "1 0.926478\n",
      "1 0.9307283\n",
      "1 0.9292557\n",
      "1 0.95531005\n",
      "1 0.9475387\n",
      "1 0.78048235\n",
      "1 0.9094829\n",
      "1 0.976149\n",
      "1 0.92968965\n",
      "1 0.8332535\n",
      "1 0.81163514\n",
      "1 0.9158538\n",
      "1 0.95221376\n",
      "1 0.7323054\n",
      "1 0.95026815\n",
      "1 0.9384236\n",
      "1 0.9649485\n",
      "1 0.95934033\n",
      "1 0.8309941\n",
      "1 0.957829\n",
      "1 0.8972788\n",
      "1 0.7936546\n",
      "1 0.760174\n",
      "1 0.93499553\n",
      "1 0.93125033\n",
      "1 0.9523282\n",
      "1 0.9570846\n",
      "1 0.95039886\n",
      "1 0.9442728\n",
      "1 0.95245636\n",
      "1 0.9568131\n",
      "1 0.95743513\n",
      "1 0.9325817\n",
      "1 0.81083566\n",
      "1 0.94574106\n",
      "1 0.77761036\n",
      "1 0.9662261\n",
      "1 0.9448796\n",
      "1 0.96319383\n",
      "1 0.9519445\n",
      "0 0.6768901\n",
      "1 0.92577016\n",
      "0 0.65707374\n",
      "1 0.93938285\n",
      "1 0.949924\n",
      "1 0.91628593\n",
      "1 0.9543498\n",
      "1 0.924655\n",
      "1 0.5281674\n",
      "1 0.599075\n",
      "0 0.68283314\n",
      "1 0.7347528\n",
      "1 0.97859037\n",
      "1 0.96308464\n",
      "1 0.935625\n",
      "1 0.5236307\n",
      "1 0.92882586\n",
      "1 0.9413676\n",
      "1 0.5953988\n",
      "1 0.91640496\n",
      "1 0.93560475\n",
      "1 0.7919912\n",
      "0 0.8009955\n",
      "1 0.7767049\n",
      "1 0.93836904\n",
      "1 0.9433982\n",
      "1 0.77523124\n",
      "1 0.9087279\n",
      "0 0.6237099\n",
      "1 0.7642627\n",
      "1 0.90130377\n",
      "1 0.891851\n",
      "1 0.95637673\n",
      "1 0.9398551\n",
      "1 0.957679\n",
      "1 0.9255446\n",
      "1 0.92389196\n",
      "1 0.8484247\n",
      "1 0.9658867\n",
      "1 0.9640759\n",
      "1 0.95515805\n",
      "0 0.62148196\n",
      "1 0.97370446\n",
      "1 0.8285943\n",
      "1 0.96821666\n",
      "1 0.9608497\n",
      "1 0.5549362\n",
      "1 0.926478\n",
      "1 0.9754077\n",
      "1 0.7798054\n",
      "1 0.9418161\n",
      "1 0.92639625\n",
      "0 0.9980489\n",
      "1 0.89644504\n",
      "1 0.9243708\n",
      "1 0.89437264\n",
      "1 0.82445663\n",
      "1 0.90392834\n",
      "1 0.96220213\n",
      "0 0.6454173\n",
      "1 0.93045443\n",
      "1 0.91926837\n",
      "1 0.9621581\n",
      "1 0.94346046\n",
      "1 0.937338\n",
      "1 0.9623855\n",
      "1 0.5406991\n",
      "1 0.9243708\n",
      "1 0.94958425\n",
      "1 0.9175596\n",
      "1 0.93704337\n",
      "1 0.55098134\n",
      "1 0.94986504\n",
      "1 0.95007616\n",
      "1 0.5281674\n",
      "0 0.91476417\n",
      "1 0.76376235\n",
      "1 0.7622041\n",
      "1 0.9383216\n",
      "1 0.93269837\n",
      "1 0.96243316\n",
      "1 0.800049\n",
      "0 0.6762367\n",
      "0 0.6528363\n",
      "1 0.9623855\n",
      "1 0.97438735\n",
      "1 0.93863875\n",
      "1 0.8073919\n",
      "1 0.9746825\n",
      "1 0.9400332\n",
      "0 0.6293308\n",
      "1 0.84304905\n",
      "1 0.9267471\n",
      "1 0.95236284\n",
      "1 0.94523966\n",
      "1 0.90352297\n",
      "1 0.9563319\n",
      "1 0.92968965\n",
      "1 0.9335424\n",
      "1 0.9442981\n",
      "1 0.942638\n",
      "1 0.96277153\n",
      "0 0.67426276\n",
      "0 0.9960756\n",
      "1 0.93232924\n",
      "1 0.7211784\n",
      "1 0.9760083\n",
      "1 0.6350929\n",
      "1 0.6399366\n",
      "1 0.5845959\n",
      "1 0.95057803\n",
      "1 0.93836904\n",
      "0 0.995276\n",
      "0 0.5671574\n",
      "0 0.8555015\n",
      "1 0.94990873\n",
      "1 0.9529995\n",
      "1 0.84722286\n",
      "1 0.6259547\n",
      "0 0.5766167\n",
      "1 0.8831671\n",
      "1 0.9633833\n",
      "1 0.95663327\n",
      "1 0.63693345\n",
      "0 0.6718681\n",
      "1 0.9552642\n",
      "1 0.9349844\n",
      "1 0.9624609\n",
      "1 0.8360022\n",
      "1 0.93342483\n",
      "1 0.94602585\n",
      "1 0.9763299\n",
      "1 0.93619776\n",
      "1 0.9658328\n",
      "1 0.941346\n",
      "1 0.59281236\n",
      "1 0.6542889\n",
      "1 0.95950174\n",
      "1 0.7640732\n",
      "1 0.58872896\n",
      "1 0.6287794\n",
      "0 0.5698182\n",
      "1 0.5779345\n",
      "1 0.91870075\n",
      "1 0.9623563\n",
      "1 0.9208369\n",
      "1 0.9753248\n",
      "1 0.93317163\n",
      "1 0.94285816\n",
      "1 0.9638996\n",
      "1 0.91726065\n",
      "1 0.95396954\n",
      "1 0.9370581\n",
      "1 0.5879895\n",
      "0 0.5959168\n",
      "1 0.81836325\n",
      "1 0.9499483\n",
      "1 0.9415614\n",
      "1 0.94821286\n",
      "1 0.9582855\n",
      "1 0.7365202\n",
      "1 0.9193561\n",
      "0 0.59139967\n",
      "1 0.95834136\n",
      "1 0.94881326\n",
      "0 0.58634764\n",
      "1 0.7378117\n",
      "1 0.7758421\n",
      "1 0.97410345\n",
      "1 0.97128546\n",
      "1 0.92551184\n",
      "1 0.77135384\n",
      "0 0.86949986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9610573\n",
      "1 0.92576903\n",
      "1 0.9591737\n",
      "0 0.9977343\n",
      "1 0.9426367\n",
      "1 0.9448513\n",
      "1 0.94510645\n",
      "1 0.9413077\n",
      "1 0.9268807\n",
      "0 0.99726665\n",
      "1 0.9024021\n",
      "1 0.9531129\n",
      "1 0.9375054\n",
      "1 0.95757973\n",
      "1 0.5208739\n",
      "0 0.92493373\n",
      "1 0.95494336\n",
      "1 0.7436743\n",
      "1 0.5313499\n",
      "1 0.9654969\n",
      "1 0.9484845\n",
      "1 0.9397154\n",
      "1 0.9675793\n",
      "1 0.6259547\n",
      "1 0.9437117\n",
      "1 0.7814851\n",
      "1 0.95095974\n",
      "1 0.95940584\n",
      "1 0.9597799\n",
      "1 0.9684705\n",
      "0 0.98515075\n",
      "1 0.95245636\n",
      "1 0.947987\n",
      "1 0.9490769\n",
      "0 0.7553852\n",
      "1 0.97533584\n",
      "1 0.9616528\n",
      "1 0.9545138\n",
      "1 0.92881155\n",
      "1 0.76628435\n",
      "1 0.89550805\n",
      "1 0.94841474\n",
      "0 0.7111561\n",
      "1 0.9530443\n",
      "1 0.9567751\n",
      "1 0.95058256\n",
      "1 0.94184715\n",
      "1 0.9579039\n",
      "1 0.9088604\n",
      "1 0.5728224\n",
      "1 0.9450524\n",
      "1 0.89097327\n",
      "0 0.9983032\n",
      "1 0.9603251\n",
      "1 0.9369089\n",
      "1 0.9760543\n",
      "0 0.6768901\n",
      "0 0.5040164\n",
      "1 0.95774674\n",
      "1 0.9573229\n",
      "1 0.79980797\n",
      "1 0.9513395\n",
      "0 0.62983257\n",
      "1 0.94488883\n",
      "1 0.9429489\n",
      "1 0.9564623\n",
      "1 0.94399005\n",
      "0 0.50605655\n",
      "1 0.61204404\n",
      "1 0.9707235\n",
      "1 0.9165627\n",
      "1 0.94967896\n",
      "1 0.85168004\n",
      "1 0.9573229\n",
      "1 0.9265218\n",
      "1 0.9274702\n",
      "1 0.9128564\n",
      "1 0.97368866\n",
      "1 0.9374575\n",
      "1 0.9680411\n",
      "1 0.9423917\n",
      "1 0.80162793\n",
      "1 0.9662261\n",
      "1 0.9554586\n",
      "1 0.75799775\n",
      "1 0.82852083\n",
      "1 0.9187107\n",
      "1 0.94325274\n",
      "1 0.963721\n",
      "1 0.9745627\n",
      "1 0.8244278\n",
      "1 0.95253086\n",
      "1 0.932028\n",
      "1 0.74096674\n",
      "1 0.957207\n",
      "0 0.7917012\n",
      "1 0.92906296\n",
      "1 0.9173108\n",
      "1 0.9577585\n",
      "1 0.9326224\n",
      "1 0.96080196\n",
      "1 0.96032304\n",
      "1 0.9512075\n",
      "1 0.97155625\n",
      "1 0.950714\n",
      "1 0.95152485\n",
      "1 0.92342836\n",
      "1 0.9243151\n",
      "1 0.91889477\n",
      "0 0.99772257\n",
      "1 0.7644155\n",
      "1 0.9627189\n",
      "0 0.9185684\n",
      "1 0.9585452\n",
      "1 0.9161985\n",
      "1 0.9639919\n",
      "1 0.959792\n",
      "1 0.97664034\n",
      "1 0.9458487\n",
      "1 0.9366062\n",
      "1 0.90854955\n",
      "1 0.9585452\n",
      "1 0.75919276\n",
      "1 0.9522062\n",
      "1 0.9613105\n",
      "1 0.9713382\n",
      "1 0.93597734\n",
      "1 0.6979515\n",
      "1 0.91059643\n",
      "1 0.9161985\n",
      "1 0.96809846\n",
      "1 0.9174184\n",
      "1 0.7436743\n",
      "1 0.9667087\n",
      "1 0.95528895\n",
      "1 0.551118\n",
      "0 0.9980394\n",
      "1 0.83182144\n",
      "1 0.94405836\n",
      "1 0.9167004\n",
      "1 0.9594664\n",
      "1 0.95568705\n",
      "1 0.9716484\n",
      "0 0.9057717\n",
      "1 0.7396503\n",
      "1 0.8621976\n",
      "1 0.9257168\n",
      "1 0.69567174\n",
      "1 0.9679473\n",
      "1 0.9249204\n",
      "1 0.9591977\n",
      "1 0.9484409\n",
      "1 0.9470349\n",
      "1 0.76994735\n",
      "1 0.9385755\n",
      "1 0.96365\n",
      "1 0.64360064\n",
      "1 0.9524796\n",
      "1 0.8971045\n",
      "1 0.53847104\n",
      "1 0.94456893\n",
      "1 0.795781\n",
      "1 0.90342546\n",
      "1 0.93930596\n",
      "1 0.967071\n",
      "1 0.90665025\n",
      "1 0.6108542\n",
      "1 0.53847104\n",
      "1 0.9385464\n",
      "1 0.9496639\n",
      "1 0.9600833\n",
      "1 0.8980235\n",
      "1 0.9520422\n",
      "1 0.94545466\n",
      "1 0.8899944\n",
      "1 0.95847875\n",
      "1 0.9746079\n",
      "0 0.6325466\n",
      "1 0.80654246\n",
      "1 0.94615734\n",
      "1 0.9262106\n",
      "0 0.9973055\n",
      "1 0.9420279\n",
      "1 0.647582\n",
      "1 0.9024021\n",
      "0 0.9981437\n",
      "0 0.6337835\n",
      "1 0.95212424\n",
      "1 0.6522701\n",
      "0 0.99837506\n",
      "1 0.93302184\n",
      "1 0.9533726\n",
      "1 0.957207\n",
      "1 0.93926275\n",
      "1 0.9523914\n",
      "1 0.96821666\n",
      "1 0.9574067\n",
      "0 0.7379462\n",
      "1 0.75765777\n",
      "1 0.7949673\n",
      "1 0.8518565\n",
      "0 0.9065754\n",
      "0 0.55166966\n",
      "1 0.9598647\n",
      "1 0.9567751\n",
      "1 0.9603726\n",
      "1 0.9708143\n",
      "1 0.94731075\n",
      "0 0.9663826\n",
      "1 0.95865005\n",
      "1 0.90905964\n",
      "1 0.9661718\n",
      "1 0.8702255\n",
      "1 0.8334552\n",
      "1 0.9382052\n",
      "1 0.94307536\n",
      "1 0.96710557\n",
      "0 0.7088066\n",
      "0 0.6735793\n",
      "1 0.9501423\n",
      "1 0.93863875\n",
      "1 0.8294291\n",
      "1 0.9218208\n",
      "1 0.96383977\n",
      "1 0.9418884\n",
      "0 0.91889983\n",
      "1 0.9607371\n",
      "1 0.6318761\n",
      "1 0.5196724\n",
      "0 0.67160285\n",
      "1 0.60625297\n",
      "1 0.9373517\n",
      "1 0.66644263\n",
      "1 0.94781154\n",
      "1 0.96278864\n",
      "1 0.9330668\n",
      "1 0.66644263\n",
      "1 0.95377785\n",
      "1 0.96148735\n",
      "1 0.5320162\n",
      "1 0.944389\n",
      "1 0.9291664\n",
      "1 0.9684911\n",
      "1 0.97729015\n",
      "1 0.93915117\n",
      "1 0.97794867\n",
      "1 0.95762604\n",
      "0 0.62148196\n",
      "1 0.92639625\n",
      "0 0.99828726\n",
      "1 0.9567751\n",
      "1 0.95058256\n",
      "0 0.6350626\n",
      "1 0.90979666\n",
      "1 0.9374575\n",
      "1 0.93198484\n",
      "1 0.96821666\n",
      "0 0.99838305\n",
      "1 0.9185015\n",
      "1 0.5118803\n",
      "0 0.6405845\n",
      "1 0.839979\n",
      "0 0.5572864\n",
      "1 0.72021794\n",
      "1 0.96739435\n",
      "1 0.7988223\n",
      "1 0.9253041\n",
      "1 0.75049984\n",
      "1 0.9675368\n",
      "1 0.93748194\n",
      "1 0.9367013\n",
      "0 0.827155\n",
      "1 0.7798054\n",
      "1 0.9552615\n",
      "1 0.9522688\n",
      "1 0.94399005\n",
      "1 0.94574106\n",
      "1 0.97155625\n",
      "1 0.893549\n",
      "1 0.9650079\n",
      "1 0.84304905\n",
      "1 0.96522886\n",
      "1 0.5504816\n",
      "1 0.95834136\n",
      "1 0.9363551\n",
      "1 0.7800812\n",
      "1 0.7767049\n",
      "1 0.9740603\n",
      "1 0.85248935\n",
      "1 0.9367013\n",
      "1 0.96380216\n",
      "1 0.97644466\n",
      "1 0.96622247\n",
      "1 0.5728224\n",
      "1 0.9457061\n",
      "1 0.9519465\n",
      "1 0.9433752\n",
      "1 0.9577868\n",
      "1 0.9535447\n",
      "1 0.8406997\n",
      "1 0.9574759\n",
      "1 0.95568705\n",
      "1 0.60849863\n",
      "1 0.507407\n",
      "0 0.9189197\n",
      "0 0.50834274\n",
      "1 0.8160911\n",
      "1 0.7687894\n",
      "1 0.9767294\n",
      "1 0.89742017\n",
      "1 0.904056\n",
      "1 0.9503069\n",
      "1 0.9645877\n",
      "1 0.9434867\n",
      "1 0.9364536\n",
      "1 0.9457061\n",
      "1 0.90979666\n",
      "1 0.88238513\n",
      "1 0.959733\n",
      "1 0.9543498\n",
      "1 0.97530156\n",
      "1 0.94958013\n",
      "1 0.922074\n",
      "1 0.82055134\n",
      "1 0.9409972\n",
      "1 0.5332182\n",
      "1 0.73580587\n",
      "1 0.9444529\n",
      "1 0.65643615\n",
      "1 0.9197134\n",
      "1 0.69483525\n",
      "1 0.95482767\n",
      "1 0.94363886\n",
      "1 0.9290441\n",
      "1 0.9667431\n",
      "1 0.9520572\n",
      "0 0.9420784\n",
      "1 0.5619153\n",
      "1 0.95999277\n",
      "1 0.92578924\n",
      "1 0.93374205\n",
      "1 0.9035162\n",
      "1 0.956922\n",
      "1 0.96427596\n",
      "1 0.6587469\n",
      "1 0.9259634\n",
      "1 0.9591687\n",
      "1 0.8555317\n",
      "1 0.93031734\n",
      "1 0.95941997\n",
      "1 0.73509485\n",
      "1 0.9448796\n",
      "1 0.72073394\n",
      "1 0.78834707\n",
      "1 0.77135384\n",
      "1 0.5953988\n",
      "1 0.8993738\n",
      "1 0.95681286\n",
      "1 0.9582611\n",
      "1 0.943525\n",
      "0 0.7111561\n",
      "1 0.8972788\n",
      "1 0.9549681\n",
      "1 0.6826317\n",
      "1 0.90214735\n",
      "1 0.9295504\n",
      "1 0.9392449\n",
      "1 0.80290776\n",
      "1 0.963384\n",
      "1 0.68971545\n",
      "1 0.8899944\n",
      "1 0.8173453\n",
      "1 0.9335995\n",
      "1 0.9662261\n",
      "1 0.76994735\n",
      "1 0.6801526\n",
      "1 0.93053114\n",
      "1 0.9600777\n",
      "1 0.9197176\n",
      "0 0.6423391\n",
      "1 0.9544268\n",
      "1 0.8728909\n",
      "1 0.5008665\n",
      "1 0.93469757\n",
      "1 0.942211\n",
      "1 0.96809846\n",
      "1 0.5196724\n",
      "1 0.7938252\n",
      "1 0.92891985\n",
      "1 0.94761646\n",
      "0 0.7111561\n",
      "1 0.965669\n",
      "1 0.9591977\n",
      "1 0.9561947\n",
      "1 0.9494003\n",
      "1 0.9213403\n",
      "1 0.7919912\n",
      "1 0.85572344\n",
      "1 0.6811422\n",
      "1 0.64360064\n",
      "1 0.67195994\n",
      "1 0.9193561\n",
      "1 0.9606623\n",
      "1 0.96710736\n",
      "1 0.9293977\n",
      "1 0.97527426\n",
      "1 0.7786014\n",
      "1 0.9292631\n",
      "1 0.9557861\n",
      "1 0.97024393\n",
      "1 0.8231055\n",
      "1 0.6259547\n",
      "1 0.91759783\n",
      "1 0.96522886\n",
      "1 0.9559702\n",
      "1 0.93985456\n",
      "1 0.94020754\n",
      "1 0.9626486\n",
      "1 0.9301936\n",
      "1 0.51406187\n",
      "0 0.7369892\n",
      "0 0.9420784\n",
      "1 0.9662261\n",
      "1 0.9389408\n",
      "1 0.5282504\n",
      "1 0.6066507\n",
      "1 0.9609156\n",
      "1 0.5011838\n",
      "1 0.9549116\n",
      "1 0.91167134\n",
      "1 0.8898741\n",
      "1 0.95940584\n",
      "1 0.6979795\n",
      "1 0.8062687\n",
      "1 0.95077956\n",
      "1 0.9193403\n",
      "1 0.95690125\n",
      "1 0.56681585\n",
      "1 0.90078247\n",
      "1 0.9292557\n",
      "1 0.6069014\n",
      "1 0.9476795\n",
      "1 0.96577924\n",
      "1 0.79191\n",
      "1 0.96668607\n",
      "1 0.9450241\n",
      "1 0.8972911\n",
      "1 0.9409972\n",
      "1 0.9198934\n",
      "1 0.97272336\n",
      "1 0.9504514\n",
      "1 0.9531325\n",
      "1 0.9317902\n",
      "1 0.94604826\n",
      "1 0.9512296\n",
      "1 0.95034134\n",
      "1 0.92882586\n",
      "1 0.96603185\n",
      "1 0.94263226\n",
      "1 0.94256425\n",
      "1 0.68973565\n",
      "1 0.91828036\n",
      "1 0.95531005\n",
      "1 0.961331\n",
      "1 0.9540025\n",
      "1 0.91923404\n",
      "1 0.9563235\n",
      "1 0.9625456\n",
      "1 0.9608364\n",
      "1 0.94957876\n",
      "1 0.95109344\n",
      "1 0.83690476\n",
      "1 0.94867474\n",
      "1 0.9708055\n",
      "1 0.6944332\n",
      "1 0.9422752\n",
      "1 0.93246156\n",
      "1 0.924655\n",
      "1 0.9348507\n",
      "1 0.7644155\n",
      "1 0.92461586\n",
      "1 0.9749782\n",
      "1 0.9609156\n",
      "1 0.9158754\n",
      "0 0.6449991\n",
      "1 0.9456058\n",
      "1 0.957182\n",
      "1 0.94087553\n",
      "1 0.96535736\n",
      "1 0.6734398\n",
      "1 0.9591687\n",
      "1 0.94743615\n",
      "1 0.67157894\n",
      "1 0.957822\n",
      "1 0.94908667\n",
      "1 0.9555216\n",
      "1 0.9553342\n",
      "1 0.66831994\n",
      "0 0.7606232\n",
      "1 0.96450806\n",
      "1 0.9512026\n",
      "1 0.96278864\n",
      "1 0.9076619\n",
      "1 0.55064136\n",
      "1 0.967401\n",
      "1 0.5471771\n",
      "1 0.94871175\n",
      "1 0.9324422\n",
      "1 0.5113856\n",
      "1 0.89746314\n",
      "1 0.5902663\n",
      "1 0.9583139\n",
      "1 0.9600833\n",
      "1 0.96248066\n",
      "0 0.69095975\n",
      "1 0.93291664\n",
      "1 0.96661466\n",
      "0 0.5006271\n",
      "1 0.9666353\n",
      "1 0.7794233\n",
      "0 0.67426276\n",
      "1 0.9302087\n",
      "1 0.9564246\n",
      "0 0.55799806\n",
      "1 0.95738953\n",
      "1 0.93213236\n",
      "1 0.71635294\n",
      "1 0.94958425\n",
      "1 0.95236284\n",
      "1 0.8570658\n",
      "1 0.9175927\n",
      "1 0.9563076\n",
      "1 0.9584332\n",
      "1 0.6826317\n",
      "1 0.9541616\n",
      "1 0.6183938\n",
      "1 0.8122537\n",
      "1 0.534289\n",
      "1 0.9414716\n",
      "1 0.935625\n",
      "1 0.9192336\n",
      "1 0.7380228\n",
      "1 0.9135296\n",
      "1 0.9606684\n",
      "1 0.7661718\n",
      "1 0.53769755\n",
      "1 0.9655114\n",
      "1 0.96083814\n",
      "1 0.8167742\n",
      "1 0.9522717\n",
      "1 0.9665754\n",
      "1 0.95940584\n",
      "1 0.94821984\n",
      "1 0.949251\n",
      "1 0.96577924\n",
      "1 0.94405836\n",
      "0 0.62983257\n",
      "1 0.9406891\n",
      "1 0.9684705\n",
      "1 0.94602585\n",
      "1 0.7997746\n",
      "1 0.9446107\n",
      "1 0.94325274\n",
      "1 0.9476795\n",
      "1 0.83896804\n",
      "1 0.9531325\n",
      "1 0.95999277\n",
      "1 0.9625456\n",
      "1 0.52056074\n",
      "1 0.9671647\n",
      "1 0.9569858\n",
      "1 0.9149979\n",
      "1 0.9591737\n",
      "0 0.5147397\n",
      "1 0.7646497\n",
      "1 0.94669324\n",
      "1 0.544128\n",
      "1 0.9420279\n",
      "1 0.78419167\n",
      "1 0.9448796\n",
      "1 0.95055455\n",
      "0 0.542147\n",
      "1 0.545018\n",
      "1 0.9749655\n",
      "1 0.9551678\n",
      "1 0.95007616\n",
      "1 0.9721611\n",
      "1 0.97328883\n",
      "0 0.67622197\n",
      "1 0.96978843\n",
      "0 0.6274627\n",
      "1 0.9380851\n",
      "1 0.81083566\n",
      "1 0.92436725\n",
      "0 0.9982021\n",
      "1 0.8966605\n",
      "1 0.8347688\n",
      "1 0.77122\n",
      "1 0.5258417\n",
      "0 0.61053467\n",
      "1 0.95873916\n",
      "0 0.58634764\n",
      "1 0.9470349\n",
      "1 0.9522717\n",
      "1 0.5173004\n",
      "1 0.8551917\n",
      "1 0.9576932\n",
      "1 0.96387935\n",
      "1 0.94353265\n",
      "0 0.9973768\n",
      "1 0.9675368\n",
      "1 0.9549116\n",
      "1 0.7451264\n",
      "1 0.96809846\n",
      "1 0.95115006\n",
      "1 0.9639919\n",
      "1 0.82834244\n",
      "1 0.967628\n",
      "1 0.9102051\n",
      "0 0.65984577\n",
      "1 0.9432651\n",
      "1 0.9529726\n",
      "1 0.84939414\n",
      "1 0.9544567\n",
      "1 0.9409083\n",
      "0 0.6051709\n",
      "1 0.50942683\n",
      "1 0.9193561\n",
      "1 0.95377666\n",
      "1 0.82484084\n",
      "1 0.9003367\n",
      "1 0.7787701\n",
      "1 0.9432651\n",
      "1 0.92342687\n",
      "1 0.9653267\n",
      "1 0.9619936\n",
      "1 0.75447786\n",
      "1 0.9359519\n",
      "1 0.84241784\n",
      "0 0.6714389\n",
      "1 0.94415313\n",
      "1 0.967065\n",
      "1 0.8980235\n",
      "1 0.87610865\n",
      "1 0.9573216\n",
      "1 0.9675368\n",
      "1 0.9242051\n",
      "1 0.9005555\n",
      "1 0.9351983\n",
      "1 0.95606685\n",
      "1 0.9749858\n",
      "1 0.97693187\n",
      "1 0.9713382\n",
      "0 0.65502787\n",
      "1 0.6609528\n",
      "1 0.92639625\n",
      "1 0.9490888\n",
      "1 0.9494854\n",
      "1 0.84011394\n",
      "1 0.97395885\n",
      "1 0.96435094\n",
      "1 0.9603251\n",
      "1 0.543368\n",
      "1 0.94884855\n",
      "0 0.7606232\n",
      "1 0.95446056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9544535\n",
      "1 0.9148918\n",
      "1 0.51407754\n",
      "1 0.50568736\n",
      "1 0.9302475\n",
      "1 0.6640626\n",
      "1 0.51510257\n",
      "1 0.90392834\n",
      "1 0.9550218\n",
      "1 0.95841646\n",
      "1 0.82792795\n",
      "1 0.9438628\n",
      "1 0.9652224\n",
      "1 0.96534383\n",
      "1 0.9499483\n",
      "1 0.92509115\n",
      "1 0.9042614\n",
      "1 0.63956034\n",
      "1 0.7875962\n",
      "1 0.93821084\n",
      "1 0.95789766\n",
      "1 0.9364877\n",
      "1 0.96037287\n",
      "1 0.89507097\n",
      "1 0.91933537\n",
      "1 0.9461941\n",
      "1 0.93779165\n",
      "1 0.9016909\n",
      "1 0.9404625\n",
      "1 0.94945276\n",
      "1 0.9585452\n",
      "0 0.9947471\n",
      "1 0.9197134\n",
      "1 0.93303925\n",
      "1 0.7985558\n",
      "1 0.9678652\n",
      "1 0.95531005\n",
      "1 0.96032304\n",
      "1 0.94895697\n",
      "1 0.91392833\n",
      "1 0.9433653\n",
      "1 0.96308464\n",
      "1 0.71786827\n",
      "1 0.9282989\n",
      "1 0.9031345\n",
      "1 0.926478\n",
      "1 0.96522886\n",
      "1 0.79271954\n",
      "1 0.7548219\n",
      "1 0.96186495\n",
      "1 0.9446504\n",
      "1 0.93200177\n",
      "1 0.8422112\n",
      "1 0.9320806\n",
      "1 0.8555317\n",
      "1 0.94957876\n",
      "1 0.9316116\n",
      "1 0.9461401\n",
      "1 0.9519445\n",
      "0 0.7359274\n",
      "1 0.91616434\n",
      "1 0.8858541\n",
      "1 0.9172457\n",
      "0 0.73898184\n",
      "1 0.94869524\n",
      "0 0.7688738\n",
      "1 0.9624695\n",
      "1 0.9567751\n",
      "1 0.9394292\n",
      "1 0.9625456\n",
      "1 0.822152\n",
      "1 0.7901918\n",
      "1 0.9301533\n",
      "1 0.95808774\n",
      "1 0.76994735\n",
      "1 0.55802333\n",
      "1 0.94363886\n",
      "1 0.61659485\n",
      "1 0.96649945\n",
      "1 0.72069085\n",
      "0 0.69438684\n",
      "1 0.9496888\n",
      "0 0.6528363\n",
      "1 0.94169575\n",
      "1 0.96913177\n",
      "0 0.99837506\n",
      "1 0.96209186\n",
      "1 0.938006\n",
      "1 0.96034324\n",
      "0 0.9947471\n",
      "1 0.91200745\n",
      "1 0.9583113\n",
      "1 0.9334883\n",
      "1 0.9448796\n",
      "1 0.9484044\n",
      "1 0.5397935\n",
      "1 0.9484845\n",
      "1 0.965471\n",
      "1 0.95879656\n",
      "1 0.9623041\n",
      "0 0.6293308\n",
      "1 0.85264707\n",
      "1 0.96576494\n",
      "1 0.95580465\n",
      "1 0.96145165\n",
      "1 0.95934033\n",
      "1 0.9455588\n",
      "1 0.6862894\n",
      "1 0.9146059\n",
      "1 0.8691696\n",
      "1 0.96248066\n",
      "1 0.8724051\n",
      "1 0.93477464\n",
      "1 0.9591975\n",
      "1 0.9544529\n",
      "1 0.9249399\n",
      "1 0.9315679\n",
      "1 0.8572479\n",
      "0 0.6547066\n",
      "1 0.84297544\n",
      "1 0.6859798\n",
      "1 0.9477922\n",
      "1 0.9453514\n",
      "1 0.9719652\n",
      "1 0.8929212\n",
      "1 0.9174102\n",
      "1 0.96617985\n",
      "1 0.56656396\n",
      "1 0.97466403\n",
      "1 0.94908667\n",
      "1 0.89857113\n",
      "0 0.64309055\n",
      "1 0.9554355\n",
      "1 0.834704\n",
      "0 0.9976794\n",
      "1 0.9549116\n",
      "1 0.96303886\n",
      "1 0.95939124\n",
      "1 0.9498233\n",
      "1 0.6077378\n",
      "1 0.7945884\n",
      "1 0.93399054\n",
      "1 0.9640759\n",
      "1 0.9456134\n",
      "1 0.9274149\n",
      "1 0.9510159\n",
      "1 0.9673551\n",
      "1 0.5173004\n",
      "1 0.9544305\n",
      "1 0.85264707\n",
      "1 0.5778545\n",
      "1 0.96562874\n",
      "1 0.6125423\n",
      "1 0.9664587\n",
      "1 0.95999277\n",
      "1 0.7726219\n",
      "1 0.9559056\n",
      "1 0.6529981\n",
      "1 0.9536874\n",
      "1 0.90747094\n",
      "1 0.96710736\n",
      "1 0.9302514\n",
      "1 0.9612797\n",
      "1 0.85540426\n",
      "1 0.97021645\n",
      "1 0.6609756\n",
      "1 0.9413077\n",
      "1 0.9328247\n",
      "1 0.9676312\n",
      "1 0.96684945\n",
      "1 0.957221\n",
      "1 0.9623855\n",
      "1 0.9677376\n",
      "1 0.9568933\n",
      "1 0.94428504\n",
      "1 0.95652753\n",
      "1 0.9448513\n",
      "1 0.58810115\n",
      "1 0.90138346\n",
      "1 0.9270855\n",
      "1 0.9531325\n",
      "1 0.9440695\n",
      "1 0.92891985\n",
      "1 0.92747563\n",
      "1 0.9175927\n",
      "1 0.93560475\n",
      "1 0.9171068\n",
      "1 0.95762604\n",
      "1 0.7397692\n",
      "1 0.9349844\n",
      "1 0.93198264\n",
      "1 0.9750117\n",
      "1 0.9574267\n",
      "1 0.913817\n",
      "1 0.9202999\n",
      "1 0.89252496\n",
      "1 0.9433108\n",
      "0 0.8415889\n",
      "1 0.932209\n",
      "1 0.9659799\n",
      "0 0.86949986\n",
      "1 0.9436135\n",
      "1 0.94841474\n",
      "0 0.58733904\n",
      "1 0.9623855\n",
      "1 0.8020556\n",
      "1 0.9442728\n",
      "1 0.9026471\n",
      "1 0.947987\n",
      "1 0.92727983\n",
      "1 0.9591614\n",
      "1 0.96116084\n",
      "1 0.83558387\n",
      "1 0.7991274\n",
      "1 0.9368411\n",
      "1 0.95580465\n",
      "0 0.6895611\n",
      "1 0.9593495\n",
      "1 0.9495056\n",
      "0 0.6619594\n",
      "1 0.57316566\n",
      "1 0.91035634\n",
      "1 0.9662375\n",
      "1 0.9172457\n",
      "1 0.90909\n",
      "1 0.7447347\n",
      "1 0.93252707\n",
      "0 0.9065754\n",
      "1 0.539301\n",
      "1 0.72131205\n",
      "1 0.96032304\n",
      "1 0.95665085\n",
      "1 0.94773895\n",
      "1 0.5291674\n",
      "1 0.5430223\n",
      "1 0.9353866\n",
      "1 0.9493502\n",
      "1 0.9276638\n",
      "1 0.94958425\n",
      "1 0.9549116\n",
      "1 0.9389947\n",
      "1 0.84939414\n",
      "1 0.96585876\n",
      "1 0.92442095\n",
      "0 0.58329433\n",
      "1 0.966296\n",
      "1 0.9585452\n",
      "1 0.95524246\n",
      "1 0.95573485\n",
      "1 0.94205534\n",
      "1 0.95525205\n",
      "1 0.9535312\n",
      "1 0.73821664\n",
      "1 0.9291664\n",
      "1 0.7956803\n",
      "1 0.95034134\n",
      "0 0.6133792\n",
      "1 0.6643507\n",
      "1 0.9666716\n",
      "1 0.92310876\n",
      "1 0.72220683\n",
      "1 0.97335863\n",
      "0 0.99807966\n",
      "1 0.8956813\n",
      "1 0.9523282\n",
      "1 0.924655\n",
      "1 0.7499976\n",
      "1 0.93252707\n",
      "1 0.92635065\n",
      "1 0.5281558\n",
      "1 0.95757973\n",
      "1 0.8359473\n",
      "1 0.96368587\n",
      "1 0.9610526\n",
      "0 0.5705201\n",
      "1 0.9470192\n",
      "1 0.93302184\n",
      "1 0.9615359\n",
      "1 0.9064918\n",
      "1 0.93284273\n",
      "1 0.97571975\n",
      "1 0.75834703\n",
      "1 0.9302495\n",
      "1 0.9714093\n",
      "1 0.7596924\n",
      "0 0.7359274\n",
      "1 0.95007616\n",
      "1 0.54631764\n",
      "1 0.95857286\n",
      "1 0.96332264\n",
      "1 0.9595991\n",
      "1 0.95814925\n",
      "1 0.95634776\n",
      "1 0.9495056\n",
      "1 0.9135296\n",
      "0 0.7111561\n",
      "1 0.77637345\n",
      "1 0.5853963\n",
      "1 0.956499\n",
      "1 0.9679808\n",
      "1 0.90380675\n",
      "0 0.99766374\n",
      "1 0.94456893\n",
      "1 0.9389418\n",
      "1 0.9560641\n",
      "1 0.9523282\n",
      "1 0.81550354\n",
      "1 0.6831845\n",
      "1 0.9256443\n",
      "1 0.9357273\n",
      "1 0.9266532\n",
      "0 0.9958248\n",
      "1 0.95970863\n",
      "1 0.94995844\n",
      "0 0.9881577\n",
      "1 0.95473987\n",
      "1 0.9210362\n",
      "1 0.93893933\n",
      "1 0.93024606\n",
      "1 0.7504488\n",
      "1 0.9175927\n",
      "1 0.9093551\n",
      "1 0.96617985\n",
      "1 0.9293692\n",
      "1 0.8548643\n",
      "1 0.95720667\n",
      "0 0.9977558\n",
      "1 0.9629595\n",
      "1 0.95325935\n",
      "1 0.95525205\n",
      "0 0.9982797\n",
      "0 0.6051709\n",
      "1 0.93803924\n",
      "0 0.9982456\n",
      "1 0.97381175\n",
      "1 0.93524456\n",
      "1 0.58810115\n",
      "1 0.8267498\n",
      "1 0.94958013\n",
      "1 0.951922\n",
      "1 0.96562874\n",
      "1 0.9519445\n",
      "1 0.93291664\n",
      "1 0.5365193\n",
      "1 0.80798376\n",
      "1 0.9559056\n",
      "1 0.9443875\n",
      "1 0.97664034\n",
      "1 0.9175629\n",
      "0 0.64967114\n",
      "1 0.9455588\n",
      "1 0.9752221\n",
      "1 0.9090254\n",
      "1 0.8798611\n",
      "1 0.92671025\n",
      "1 0.9253041\n",
      "1 0.72721314\n",
      "1 0.9742668\n",
      "1 0.9324865\n",
      "0 0.68283314\n",
      "0 0.6637449\n",
      "0 0.9979875\n",
      "1 0.9533468\n",
      "1 0.9501609\n",
      "1 0.7541169\n",
      "1 0.95996237\n",
      "1 0.9128564\n",
      "1 0.7982953\n",
      "0 0.5375425\n",
      "1 0.94363886\n",
      "1 0.94954747\n",
      "1 0.7424678\n",
      "1 0.94218624\n",
      "1 0.95865005\n",
      "1 0.9281515\n",
      "0 0.6762367\n",
      "0 0.9972492\n",
      "0 0.8478306\n",
      "1 0.9650574\n",
      "1 0.7754092\n",
      "1 0.90664816\n",
      "1 0.93525594\n",
      "1 0.74210984\n",
      "1 0.91610366\n",
      "1 0.93291664\n",
      "1 0.9426475\n",
      "1 0.9407332\n",
      "1 0.95510066\n",
      "1 0.7422364\n",
      "1 0.97545797\n",
      "0 0.54035753\n",
      "0 0.9982017\n",
      "1 0.96319103\n",
      "1 0.93988955\n",
      "1 0.8350698\n",
      "1 0.9523724\n",
      "0 0.50834274\n",
      "1 0.9697175\n",
      "1 0.77200484\n",
      "1 0.8927013\n",
      "1 0.91665024\n",
      "1 0.96173286\n",
      "1 0.94638455\n",
      "1 0.93748194\n",
      "1 0.9640469\n",
      "1 0.9553144\n",
      "1 0.9393955\n",
      "0 0.50054574\n",
      "1 0.95988977\n",
      "1 0.93390673\n",
      "1 0.97070307\n",
      "1 0.9519316\n",
      "1 0.949251\n",
      "1 0.8197157\n",
      "1 0.8240586\n",
      "1 0.9602474\n",
      "1 0.90130377\n",
      "1 0.9542032\n",
      "1 0.9749782\n",
      "1 0.6859798\n",
      "1 0.6857017\n",
      "1 0.7104144\n",
      "1 0.66644263\n",
      "1 0.9664278\n",
      "1 0.8128738\n",
      "1 0.76404166\n",
      "1 0.90776616\n",
      "1 0.61659485\n",
      "1 0.9433108\n",
      "1 0.9761548\n",
      "1 0.9435075\n",
      "1 0.9016208\n",
      "1 0.5973279\n",
      "1 0.96078193\n",
      "1 0.95531005\n",
      "1 0.78002733\n",
      "1 0.95081997\n",
      "1 0.82361746\n",
      "1 0.95701414\n",
      "0 0.64421743\n",
      "1 0.79803455\n",
      "1 0.97565234\n",
      "1 0.8204156\n",
      "0 0.94502056\n",
      "1 0.94854754\n",
      "1 0.92086\n",
      "1 0.9517954\n",
      "1 0.90906507\n",
      "1 0.9407332\n",
      "1 0.9659799\n",
      "1 0.9349473\n",
      "1 0.9487222\n",
      "1 0.9630903\n",
      "1 0.72649133\n",
      "1 0.963617\n",
      "1 0.9183825\n",
      "1 0.9307283\n",
      "0 0.9978084\n",
      "1 0.95834136\n",
      "1 0.9570846\n",
      "1 0.9584711\n",
      "1 0.95439833\n",
      "1 0.9228008\n",
      "1 0.97491246\n",
      "1 0.966967\n",
      "1 0.9680411\n",
      "0 0.9058533\n",
      "1 0.96168035\n",
      "1 0.8357781\n",
      "1 0.957182\n",
      "1 0.7950466\n",
      "1 0.80290776\n",
      "1 0.92086\n",
      "1 0.96914554\n",
      "1 0.954092\n",
      "0 0.63744044\n",
      "1 0.9445582\n",
      "1 0.97440594\n",
      "1 0.9359057\n",
      "1 0.83095133\n",
      "1 0.6831845\n",
      "1 0.9558664\n",
      "1 0.71765655\n",
      "0 0.6451593\n",
      "1 0.92049146\n",
      "1 0.8968745\n",
      "1 0.95749164\n",
      "1 0.5845959\n",
      "1 0.8072912\n",
      "1 0.9743895\n",
      "1 0.9653267\n",
      "1 0.95850384\n",
      "1 0.7646497\n",
      "1 0.83868134\n",
      "1 0.96823555\n",
      "1 0.9094801\n",
      "1 0.95042676\n",
      "1 0.9651168\n",
      "1 0.54901683\n",
      "1 0.714449\n",
      "1 0.96177053\n",
      "1 0.95092255\n",
      "1 0.95883524\n",
      "1 0.96204066\n",
      "1 0.63045555\n",
      "1 0.9459044\n",
      "1 0.51407754\n",
      "1 0.9090783\n",
      "1 0.9637179\n",
      "1 0.65181714\n",
      "1 0.9550735\n",
      "1 0.7067256\n",
      "0 0.9951444\n",
      "1 0.9433653\n",
      "1 0.9414063\n",
      "1 0.9671426\n",
      "1 0.84047824\n",
      "1 0.96220213\n",
      "1 0.6117261\n",
      "1 0.9487245\n",
      "1 0.534289\n",
      "1 0.7699222\n",
      "1 0.95446056\n",
      "1 0.8122534\n",
      "1 0.9591975\n",
      "1 0.8673244\n",
      "1 0.95860404\n",
      "1 0.9329513\n",
      "1 0.9397154\n",
      "0 0.98074603\n",
      "1 0.9467779\n",
      "1 0.800861\n",
      "1 0.8937927\n",
      "1 0.95439833\n",
      "1 0.9627189\n",
      "1 0.774681\n",
      "0 0.99837506\n",
      "1 0.96380216\n",
      "1 0.94957876\n",
      "1 0.9468206\n",
      "1 0.78834707\n",
      "1 0.93399054\n",
      "1 0.9515094\n",
      "1 0.9253041\n",
      "1 0.9376017\n",
      "1 0.9554355\n",
      "1 0.95757973\n",
      "1 0.6035793\n",
      "1 0.96617985\n",
      "1 0.92461586\n",
      "1 0.93477464\n",
      "0 0.6396964\n",
      "1 0.94034976\n",
      "1 0.8712793\n",
      "1 0.9653405\n",
      "1 0.96577704\n",
      "1 0.86991006\n",
      "1 0.95042676\n",
      "1 0.95212793\n",
      "1 0.7209825\n",
      "0 0.6730205\n",
      "1 0.9564246\n",
      "1 0.52122504\n",
      "1 0.9504053\n",
      "1 0.95880127\n",
      "1 0.91692156\n",
      "1 0.9655425\n",
      "1 0.6183938\n",
      "1 0.9485014\n",
      "1 0.9293977\n",
      "1 0.95042676\n",
      "1 0.82116437\n",
      "1 0.9134164\n",
      "1 0.97286147\n",
      "1 0.9242238\n",
      "1 0.963617\n",
      "1 0.95641154\n",
      "1 0.9101349\n",
      "1 0.9446107\n",
      "1 0.76459044\n",
      "1 0.93419635\n",
      "1 0.9337058\n",
      "1 0.95026815\n",
      "1 0.8017914\n",
      "1 0.9619928\n",
      "0 0.50679463\n",
      "1 0.96006817\n",
      "1 0.9316116\n",
      "1 0.96718574\n",
      "1 0.9398634\n",
      "1 0.8955869\n",
      "1 0.7796304\n",
      "1 0.9659799\n",
      "1 0.93499553\n",
      "1 0.9593051\n",
      "1 0.96653426\n",
      "1 0.9417592\n",
      "0 0.65707374\n",
      "1 0.9535925\n",
      "1 0.9612088\n",
      "1 0.9413744\n",
      "1 0.66118544\n",
      "1 0.9697175\n",
      "0 0.58733904\n",
      "1 0.7347528\n",
      "1 0.91889477\n",
      "1 0.73245716\n",
      "1 0.795781\n",
      "1 0.9256683\n",
      "0 0.89565384\n",
      "1 0.9293692\n",
      "1 0.9448513\n",
      "1 0.9430321\n",
      "1 0.5439895\n",
      "1 0.9047427\n",
      "1 0.9622791\n",
      "1 0.75702095\n",
      "1 0.9737446\n",
      "1 0.96315587\n",
      "0 0.9947471\n",
      "1 0.92639625\n",
      "1 0.9660076\n",
      "1 0.9320399\n",
      "1 0.9388293\n",
      "1 0.8803365\n",
      "1 0.9525748\n",
      "1 0.9470192\n",
      "1 0.94428504\n",
      "1 0.9442355\n",
      "1 0.7853567\n",
      "1 0.9653538\n",
      "0 0.84558684\n",
      "1 0.5391365\n",
      "1 0.788\n",
      "1 0.951818\n",
      "1 0.96142244\n",
      "1 0.9401495\n",
      "1 0.94734263\n"
     ]
    }
   ],
   "source": [
    "### 画出标签为1的热力图，但模型的判断结果不一定为1\n",
    "for i in positive :\n",
    "    img=x_test[i].reshape(1,50,50,1)\n",
    "    path = '../heatmap/heatmaptest332/'\n",
    "    heatmapcam(img,i,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 这是对plt.matshow(znew, cmap='jet')中参数jet的解释，值越大，色调越暖，\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mat = np.arange(1, 10).reshape(3, 3)\n",
    "\n",
    "plt.matshow(mat, cmap=plt.cm.jet)\n",
    "for i in range(mat.shape[0]):\n",
    "    for j in range(mat.shape[1]):\n",
    "        plt.text(x=j, y=i, s=mat[i, j])\n",
    "               \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 40, 40, 32)        3904      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        165952    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               61560     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 170       \n",
      "=================================================================\n",
      "Total params: 315,606\n",
      "Trainable params: 315,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       0.97      0.95      0.96      6872\n",
      "       flare       0.88      0.92      0.90      2633\n",
      "\n",
      "    accuracy                           0.94      9505\n",
      "   macro avg       0.92      0.94      0.93      9505\n",
      "weighted avg       0.94      0.94      0.94      9505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "model = load_model(\"./LeNet_batch_size_512_epochs_10_kernel_size_1193_accuracy_0.9170765280723572.h5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "true_y=np.apply_along_axis(np.argmax,1,y_test)\n",
    "\n",
    "pred_y=model.predict(x_test)\n",
    "pred_y=np.apply_along_axis(np.argmax,1,pred_y)\n",
    "\n",
    "print(classification_report(true_y,pred_y,target_names=['other','flare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 42, 42, 32)        2624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 64)        100416    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 128)         131200    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               61560     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 170       \n",
      "=================================================================\n",
      "Total params: 306,134\n",
      "Trainable params: 306,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       0.96      0.98      0.97      6872\n",
      "       flare       0.94      0.89      0.92      2633\n",
      "\n",
      "    accuracy                           0.95      9505\n",
      "   macro avg       0.95      0.94      0.94      9505\n",
      "weighted avg       0.95      0.95      0.95      9505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "model = load_model(\"./LeNet_batch_size_512_epochs_10_kernel_size_974_accuracy_0.9430344700813293.h5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "true_y=np.apply_along_axis(np.argmax,1,y_test)\n",
    "\n",
    "pred_y=model.predict(x_test)\n",
    "pred_y=np.apply_along_axis(np.argmax,1,pred_y)\n",
    "\n",
    "print(classification_report(true_y,pred_y,target_names=['other','flare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 44, 44, 32)        1600      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 22, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 19, 19, 64)        32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 120)               245880    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 170       \n",
      "=================================================================\n",
      "Total params: 323,542\n",
      "Trainable params: 323,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       0.95      0.96      0.96      6872\n",
      "       flare       0.90      0.87      0.88      2633\n",
      "\n",
      "    accuracy                           0.93      9505\n",
      "   macro avg       0.92      0.91      0.92      9505\n",
      "weighted avg       0.93      0.93      0.93      9505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "model = load_model(\"./LeNet_batch_size_512_epochs_10_kernel_size_742_accuracy_0.919864296913147.h5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "true_y=np.apply_along_axis(np.argmax,1,y_test)\n",
    "\n",
    "pred_y=model.predict(x_test)\n",
    "pred_y=np.apply_along_axis(np.argmax,1,pred_y)\n",
    "\n",
    "print(classification_report(true_y,pred_y,target_names=['other','flare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 64)        32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               245880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 170       \n",
      "=================================================================\n",
      "Total params: 363,734\n",
      "Trainable params: 363,734\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       0.97      0.97      0.97      6872\n",
      "       flare       0.92      0.92      0.92      2633\n",
      "\n",
      "    accuracy                           0.95      9505\n",
      "   macro avg       0.94      0.94      0.94      9505\n",
      "weighted avg       0.96      0.95      0.96      9505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "model = load_model(\"./LeNet_batch_size_512_epochs_10_kernel_size_543_accuracy_0.9357756972312927.h5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "true_y=np.apply_along_axis(np.argmax,1,y_test)\n",
    "\n",
    "pred_y=model.predict(x_test)\n",
    "pred_y=np.apply_along_axis(np.argmax,1,pred_y)\n",
    "\n",
    "print(classification_report(true_y,pred_y,target_names=['other','flare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 38, 38, 32)        5440      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 64)        131136    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 120)               61560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 170       \n",
      "=================================================================\n",
      "Total params: 282,326\n",
      "Trainable params: 282,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       0.93      0.99      0.96      6872\n",
      "       flare       0.96      0.80      0.87      2633\n",
      "\n",
      "    accuracy                           0.94      9505\n",
      "   macro avg       0.94      0.89      0.92      9505\n",
      "weighted avg       0.94      0.94      0.93      9505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"./LeNet_batch_size_512_epochs_10_kernel_size_1383_accuracy_0.9205217957496643.h5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "true_y=np.apply_along_axis(np.argmax,1,y_test)\n",
    "\n",
    "pred_y=model.predict(x_test)\n",
    "pred_y=np.apply_along_axis(np.argmax,1,pred_y)\n",
    "\n",
    "print(classification_report(true_y,pred_y,target_names=['other','flare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 36, 36, 32)        7232      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 64)        100416    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 120)               61560     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 170       \n",
      "=================================================================\n",
      "Total params: 253,398\n",
      "Trainable params: 253,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       0.91      0.99      0.95      6872\n",
      "       flare       0.97      0.74      0.84      2633\n",
      "\n",
      "    accuracy                           0.92      9505\n",
      "   macro avg       0.94      0.87      0.90      9505\n",
      "weighted avg       0.93      0.92      0.92      9505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"./LeNet_batch_size_512_epochs_10_kernel_size_1573_accuracy_0.9118428230285645.h5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "true_y=np.apply_along_axis(np.argmax,1,y_test)\n",
    "\n",
    "pred_y=model.predict(x_test)\n",
    "pred_y=np.apply_along_axis(np.argmax,1,pred_y)\n",
    "\n",
    "print(classification_report(true_y,pred_y,target_names=['other','flare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 34, 34, 32)        9280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 17, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 10, 10, 64)        131136    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 4, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 120)               61560     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 170       \n",
      "=================================================================\n",
      "Total params: 245,206\n",
      "Trainable params: 245,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       0.96      0.97      0.96      6872\n",
      "       flare       0.92      0.89      0.90      2633\n",
      "\n",
      "    accuracy                           0.95      9505\n",
      "   macro avg       0.94      0.93      0.93      9505\n",
      "weighted avg       0.95      0.95      0.95      9505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"./LeNet_batch_size_512_epochs_10_kernel_size_1782_accuracy_0.9196538925170898.h5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "true_y=np.apply_along_axis(np.argmax,1,y_test)\n",
    "\n",
    "pred_y=model.predict(x_test)\n",
    "pred_y=np.apply_along_axis(np.argmax,1,pred_y)\n",
    "\n",
    "print(classification_report(true_y,pred_y,target_names=['other','flare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowCPU",
   "language": "python",
   "name": "tensorflowcpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
